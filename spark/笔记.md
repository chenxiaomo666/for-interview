## 背景

视频：https://www.bilibili.com/video/BV1Jq4y1z7VP/?spm_id_from=333.337.search-card.all.click&vd_source=59101a7b5e6403f5c3656d1e6ac848cb

当前进度：P14

## 一、Spark基础入门

### 第一章

#### Spark的factor

* Spark是基于内存做计算的，弹性分布式数据集（即RDD）的概念：RDD是一种分布式内存抽象，其使得程序员能够在大规模集群中做内存运算，并且有一定的容错方式。而这也是整个Spark的核心数据结构，Spark整个平台都围绕着RDD进行。

* Spark支持pandas

* 速度快

  > 1、spark在处理数据时，可以将中间处理结果数据存储到内存中（内存迭代计算）；
  >
  > 2、Spark提供了非常丰富的算子(API)，可以做到复杂任务在一个Spark程序中完成。

* 运行方式

  > Spark支持多种运行方式，包括在Hadoop和Mesos上，也支持Standalone的独立运行模式。
  >
  > 对于数据而言，Spark支持从HDFS、HBase、Cassandra及kafka等多种途径获取数据

* Spark框架模块

  > 1、SparkCore：Spark的核心，Spark核心公告均有Spark Core模块提供，是Spark运行的基础。Spark Core以RDD为数据抽象，提供python、java、scala、R语言的API，可以编程进行海量离线数据批处理计算；
  >
  > 2、SparkSQL：基于SparkCore之上，提供**结构化**数据处理模块。SparkSQL支持以SQL语言对数据进行处理，SparkSQL本身针对离线计算场景。同时基于SparkSQL，Spark提供了StructuredStreaming模块（真正的流计算？），可以以SparkSQL为基础，进行数据的流式计算。
  >
  > 3、SparkStreaming：以SpakCore为基础，提供数据的流式计算（是一个微小时间段的批处理）。
  >
  > 4、MLib：以SparkCore为基础，进行机器学习计算，内置了大量的机器学习库和API算法等。方便用户以分布式计算的模式进行机器学习计算。
  >
  > 5、GraphX：以SparkCore为基础，进行图计算，提供了大量的图计算API，方便用于分布式计算模式进行图计算。

* **Spark的运行模式（面试也遇到过）**

  > master和worker
  >
  > 1、本地模式（单机）：也被称为Local模式，用于开发和测试环境。本地模式就是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时环境。
  >
  > 2、Standalone模式（集群）：Spark中的各个角色以独立进程的形式存在，并组成Spark集群环境。
  >
  > 3、Hadoop YARN模式（集群）：Spark中的各个角色运行在YARN的容器内部，并组成Spark集群环境。
  >
  > 4、Kubernetes模式（容器集群）：Spark中的各个角色运行在Kubernetes的容器内部，并组成Spark集群环境。

#### Spark的架构角色

YARN角色回顾：

YARN主要有4类角色，从2个层面去看。

![image-20231223160324381](https://20178666.oss-cn-beijing.aliyuncs.com/img/image-20231223160324381.png)

资源管理层面：

* 集群资源管理者（Master）：ResourceManager
* 单机资源管理者（Worker）：NodeManager

任务计算层面：

* 单任务管理者（Master）:ApplicationMaster
* 单任务执行者（Worker）：Task（容器内计算框架的工作角色）

Spark运行角色：

![image-20231223160725975](https://20178666.oss-cn-beijing.aliyuncs.com/img/image-20231223160725975.png)

资源层面：

* master：集群资源管理
* worker：单机资源管理

任务运行层面：

* driver：单个任务管理
* executor：单个任务的计算（worker 干活的）

> 注：正常情况下Executor是干活的角色，不过在特殊场景下（Local模式）Driver可以即管理又干活。

#### （重点：面试常见）Spark和Hadoop（MapReduce）的对比

![image-20231223104636681](https://20178666.oss-cn-beijing.aliyuncs.com/img/image-20231223104636681.png)

* 类型：hadoop是基础平台，包含计算、存储和调度；spark是一个纯计算工具
* 场景：hadoop是海量数据批处理（基于磁盘迭代计算）；spark是海量数据的批处理（内存迭代计算、交互式计算）、海量数据流计算。
* 价格：hadoop对机器要求低，便宜；spark对内存有要求，相对较贵
* 编程范式：hadoop是map+reduce，API较为底层，算法适应性差；spark是采用RDD组成有向无环图，API较为顶层，方便使用。
* 数据存储结构：MapReduce中间计算结果在HDFS磁盘上，延迟比较大；spark的RDD中间运算结果在内存中，延迟小。
* 运行方式：hadoop的task以进程方式维护，任务启动慢；spark的task以线程方式维护，任务启动快，可批量创建提高并行能力。

#### 为什么Spark相对于Hadoop而言有较大的优势，但是Spark并不能完全替代Hadoop呢？

* 在计算层面，Spark相比较于MapReduce有巨大的性能优势，但是至今仍有很多计算工具基于MapReduce框架，比如非常成熟的Hive。
* Spark仅做计算，而Hadoop生态圈不仅有计算（MR）也有存储（HDFS）和资源管理调度（YARN），HDFS和YARN仍是许多大数据体系的核心架构。

### 第二章

#### 2.1 课程服务器环境

他要搭建三台Linux虚拟机服务器。做集群。但是我就准备在windows本机上做一个local环境，能写能跑代码就行，不然搭建环境又是十分麻烦费时的工作。继续启动。

#### 2.2 Local模式基本原理

本质：启动一个JVM Process进程（一个进程里面有多个线程），执行任务Task

Spark中Task以线程Thread方式进行

每个Task运行需要1 Core Cpu

* Local模式可以限制模拟Spark集群环境的线程数量，即Local[N]或者Local[*]
* 其中N代表可以使用N个线程，每个线程拥有一个cpu core。如果不指定N，则默认是1个线程（该线程有1个core）。通常Cpu有几个核就指定几个线程，最大化利用计算能力。
* 如果是local[*]，则代表Run Spark locally with as many worker threads as logical cores on your machine.按照cpu最多的cores设置线程数。

Local下的角色分布：

资源管理

* master：Local进程本身
* worker：Local进程本身

任务执行

Driver：Local进程本身

Executor：不存在，没有独立的Executor角色，由Local进程(也就是Driver）内的线程提供计算能力。

> 注：Driver也算一种特殊的Executor，只不过多数时候，我们将Executor当作纯Worker对待，这样额Driver好区分（一类是管理，一类是工人）
>
> 另：Local模式只能运行一个Spark程序，如果执行多个Spark程序，那就是由多个相互独立的Local进程在执行。

#### 2.3 安装包下载

#### 2.4 基础操作

#### 2.5 测试

### 第三章

### 第四章

### 第五章

### 第六章

### 第七章

### 第八章

## 二、SparkCore

### 第一章

### 第二章

### 第三章

### 第四章

### 第五章

### 第六章

## 三、SparkSQL

### 第一章

### 第二章

### 第三章

### 第四章

### 第五章

### 第六章

### 第七章

## 四、案例

### 案例背景

### 需求一开发

### 需求二开发

### 需求三开发

### 需求四开发

## 五、Spark新特性及核心回顾

### 第一章

### 第二章

### 第三章